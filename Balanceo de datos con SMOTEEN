pip install gdown
pip install pandas
pip install imblearn
------------------------------------------------------------------------------------------------------------------------------------------------------------------
import gdown
import pandas as pd
import os

# ID del archivo de Google Drive
file_id = '1z7UUXiWAvR2D4XvdcfXzkhafU2km4HM-'
# URL de descarga directa
url = f'https://drive.google.com/uc?export=download&id={file_id}'

# Ruta de destino en el nuevo disco montado
output_path = '/mnt/mydisk/BBDD_FRAUDE_PREPROCESADA_VARIABLES_DEF.csv'

# Descargar el archivo
gdown.download(url, output_path, quiet=False)

# Verificar que el archivo se ha descargado correctamente
if os.path.exists(output_path):
    print("El archivo se ha descargado correctamente en", output_path)
    
    # Leer el archivo CSV en un DataFrame de pandas
    bbdd = pd.read_csv(output_path)
    
    # Verificar que el DataFrame se ha cargado correctamente
    print("El archivo CSV se ha leído correctamente en un DataFrame de pandas.")
    display(bbdd.head())  # Mostrar las primeras filas del DataFrame para verificar
else:
    print("Error en la descarga del archivo")
#------------------------------------------------------------------------------------------------------------------------------------------------------------------
import numpy as np
bbdd['log_F_AMOUNT_TRANSACTION'] = np.log1p(bbdd['F_AMOUNT_TRANSACTION'])
bbdd['log_F_AMOUNT_TRANSACTION'] = bbdd['log_F_AMOUNT_TRANSACTION'].astype('float64')

bbdd['log_MONTO_PROM_ULT_MES'] = np.log1p(bbdd['MONTO_PROM_ULT_MES'])
bbdd['log_MONTO_PROM_ULT_MES'] = bbdd['log_MONTO_PROM_ULT_MES'].astype('float64')

bbdd['log_MONTO_PROM_ULT_MES_ENTRY'] = np.log1p(bbdd['MONTO_PROM_ULT_MES_ENTRY'])
bbdd['log_MONTO_PROM_ULT_MES_ENTRY'] = bbdd['log_MONTO_PROM_ULT_MES_ENTRY'].astype('float64')

bbdd['log_MONTO_PROM_ULT_MES_MERCHANT'] = np.log1p(bbdd['MONTO_PROM_ULT_MES_MERCHANT'])
bbdd['log_MONTO_PROM_ULT_MES_MERCHANT'] = bbdd['log_MONTO_PROM_ULT_MES_MERCHANT'].astype('float64')

bbdd['log_last_transaction_amount'] = np.log1p(bbdd['last_transaction_amount'])
bbdd['log_last_transaction_amount'] = bbdd['log_last_transaction_amount'].astype('float64')
#------------------------------------------------------------------------------------------------------------------------------------------------------------------
# Columnas a eliminar
columns_to_remove = ['PERIODO', 'ID_TRX','N_ID_CUSTOMER','S_CARD_ACCEPT_EXT_ID_CODE',
                     'F_AMOUNT_TRANSACTION','D_TRANSMISSION_DATE_AND_TIME',
                     'S_RESPONSE_CODE','NUMERO_DOCUMENTO_CODIFICADO',
                     'S_PAN','S_PAN_CODIFICADO','MONTO_PROM_ULT_MES','MONTO_PROM_ULT_MES_ENTRY'
                      'MONTO_PROM_ULT_MES_MERCHANT','last_transaction_amount']

# Método 1: List Comprehension
filtered_columns = [col for col in bbdd.columns if col not in columns_to_remove]
#------------------------------------------------------------------------------------------------------------------------------------------------------------------
bbdd['time_since_last_transaction'] = bbdd['time_since_last_transaction'].fillna(0)
bbdd['log_last_transaction_amount'] = bbdd['log_last_transaction_amount'].fillna(0)
bbdd['CANT_CLI_DISPOSITIVO'] = bbdd['CANT_CLI_DISPOSITIVO'].fillna(0)
#------------------------------------------------------------------------------------------------------------------------------------------------------------------
for col in filtered_columns:
    if 'int' in str(bbdd[col].dtype):
        bbdd[col] = bbdd[col].astype('int64')
    elif 'float' in str(bbdd[col].dtype):
        bbdd[col] = bbdd[col].astype('float64')

#------------------------------------------------------------------------------------------------------------------------------------------------------------------
import pandas as pd
import numpy as np

# Suponiendo que tienes tu dataset cargado en un DataFrame llamado df
# Asegúrate de preparar tus datos adecuadamente según las características específicas de tu dataset

# Usar dask para manejar grandes volúmenes de datos
# import dask.dataframe as dd

# Convertir pandas DataFrame a dask DataFrame
# ddf = dd.from_pandas(bbdd[filtered_columns], npartitions=1)  # Aumentar npartitions si es necesario

# Separar características (X) y etiquetas (y)
X = bbdd[filtered_columns].drop('fraude', axis=1)
y = bbdd[filtered_columns]['fraude']

# Convertir dask DataFrame a pandas DataFrame por particiones para el muestreo
# X = X.compute()
# y = y.compute()

# Aplicar NearMiss en partes para manejar la memoria
chunk_size = 1000000  # Ajusta este tamaño según la memoria disponible
num_chunks = int(np.ceil(len(X) / chunk_size))
num_chunks

------------------------------------------------------------------------------------------------------------------------------------------------------------------
import gc
del bbdd
gc.collect()


------------------------------------------------------------------------------------------------------------------------------------------------------------------
import cudf
import numpy as np
import pandas as pd
from imblearn.over_sampling import RandomOverSampler
from sklearn.datasets import make_classification

# Crear el muestreador de oversampling
sampler = RandomOverSampler()

# Lista para almacenar los resultados de oversampling
X_resampled_all = []
y_resampled_all = []

# Procesar cada chunk
for i in range(num_chunks):
    start_index = i * chunk_size
    end_index = min((i + 1) * chunk_size, len(X))
    
    # Extraer el chunk
    X_chunk = X.iloc[start_index:end_index]
    y_chunk = y.iloc[start_index:end_index]

    initial_count = len(y_chunk)
    initial_distribution = y_chunk.value_counts()
    
    # Convertir a numpy arrays
    X_np = X_chunk.to_numpy()
    y_np = y_chunk.to_numpy()
    
    # Aplicar oversampling
    X_resampled_np, y_resampled_np = sampler.fit_resample(X_np, y_np)

    final_count = len(y_resampled_np)
    final_distribution = pd.Series(y_resampled_np).value_counts()

    # Imprimir información sobre el chunk procesado
    print('*'*100)
    print(f"Chunk {i + 1}/{num_chunks}")
    print(f"Initial count: {initial_count}")
    print(f"Initial distribution:\n{initial_distribution}")
    print(f"Final count: {final_count}")
    print(f"Final distribution:\n{final_distribution}")
    print('*'*100)
    
    # Convertir los resultados a cudf DataFrame/Series
    X_resampled_cudf = cudf.DataFrame.from_records(X_resampled_np, columns=X_chunk.columns)
    y_resampled_cudf = cudf.Series(y_resampled_np)
    
    # Convertir de vuelta a pandas DataFrame/Series y almacenar en listas
    X_resampled_all.append(X_resampled_cudf.to_pandas())
    y_resampled_all.append(y_resampled_cudf.to_pandas())

# Combinar los resultados de oversampling
X_resampled_all = pd.concat(X_resampled_all, ignore_index=True)
y_resampled_all = pd.concat(y_resampled_all, ignore_index=True)
